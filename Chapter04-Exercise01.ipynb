{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/HDAT9500Banner.PNG)\n",
    "<br>\n",
    "\n",
    "# Chapter 4: Tree Based Methods\n",
    "# Exercise 01: Decision Trees and Gradient Boosting Trees\n",
    "\n",
    "\n",
    "# 1. Introduction\n",
    "\n",
    "In this exercise, we will introduce tree based methods. First, we will learn about the basic decision tree, then we will see how decision trees performance can be improved via ensemble methods - specifically, gradient descent boosting.\n",
    "\n",
    "\n",
    "## 1.1. Aims of the Exercise:\n",
    " 1. To introduce the single Decision Tree, as well as the Gradient Boosted Trees.\n",
    " 2. To explore parameters and determine appropriate choices.\n",
    "\n",
    " \n",
    "It aligns with all the learning outcome of our course: \n",
    "\n",
    "1.\tDistinguish a range of task specific machine learning techniques appropriate for Health Data Science.\n",
    "2.\tDesign machine learning tasks for Health Data Science scenarios.\n",
    "3.\tConstruct appropriate training and test sets for health research data.\n",
    "\n",
    "\n",
    "## 1.2. Jupyter Notebook Intructions\n",
    "1. Read the content of each cell.\n",
    "2. Where necessary, follow the instructions that are written in each cell.\n",
    "3. Run/Execute all the cells that contain Python code sequentially (one at a time), using the \"Run\" button.\n",
    "4. For those cells in which you are asked to write some code, please write the Python code first and then execute/run the cell.\n",
    " \n",
    "## 1.3. Tips\n",
    " 1. The square brackets on the left hand side of each cell indicate whether the cell has been executed or not. Empty square brackets mean that the cell has not been executed, whereas square brackets that contain a number means that the cell has been executed. Run all the cells in sequence, using the \"Run\" button.\n",
    " 2. To edit this notebook, just double-click in each cell. In the document, each cell can be a \"Code\" cell or \"text-Markdown\" cell. To choose between these two options, go to the combo-box above. \n",
    " 3. If you want to save your notebook, please make sure you press \"the floppy disk\" icon button above. \n",
    " 4. To clean the content of all cells and re-start Notebook, please go to Cell->All Output->Clear\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load the Wisconsin Cancer Data Set\n",
    "\n",
    "For data dictionary and all information:\n",
    "https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">**Start Activity 1**</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 1:  Can you briefly describe the dataset in 10 lines? What are we trying to predict?  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Write your answer here:</b>\n",
    "#####################################################################################################################\n",
    "\n",
    "(Double-click here)\n",
    "\n",
    "\n",
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">**End Activity 1**</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images look like this:\n",
    "\n",
    "    1. ftp://ftp.cs.wisc.edu/math-prog/cpo-dataset/machine-learn/cancer/cancer_images/ \n",
    "    2. http://pages.cs.wisc.edu/~olvi/uwmp/cancer.html#diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "#For this notebook to work, Python must be 3.6.4 or 3.6.5\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from plotnine import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = pd.read_csv('data/breast-cancer-wisconsin-data/data.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check:\n",
    "display(cancer[:][:5])\n",
    "print(cancer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cancer.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">**Start Activity 2**</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 2:  Which variable could you directly drop? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Write your answer here:</b>\n",
    "#####################################################################################################################\n",
    "\n",
    "(Double-click here)\n",
    "\n",
    "\n",
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Python Code here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "# cancer.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">**End Activity 2**</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.Split the data into features and response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cancer.drop(['diagnosis'], axis = 1)\n",
    "y = cancer[['diagnosis']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "display(X[:][:5])\n",
    "display(y[:][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Split the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Decision Trees\n",
    "Decision Trees involve segmenting the feature space (the space of our predictor variables) into a number of different regions. The method can be used for both regression (predicting numeric response variable) and classification (classifying a categorical response variable). As the set of splitting rules used to segment the feature space can be summarized into a hierarchy of if/else statements in the form of a tree, these types of approaches are known as decision tree methods.<p>\n",
    "    In the case of **regression**, in order to make a prediction for any particular observation, we usually use the mean of the training observations in the region to which it belongs. For **classification**, we usually use the mode of the training observations in the region to which the data point belongs. Recall that the mode is the most frequent element. So, *for classification we predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs*.<p>\n",
    "        We will use tree methods to predict readmission, which is a classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Growing a Tree\n",
    "Beginning at what is known as the *root node*, the node containing the entire dataset, we split the data based on the feature that provides the most information about the response variable. The split is achieved by using a *test*. The form of the test depends on the data type of the chosen feature.\n",
    "* If the feature of choice is continuous, the test will be of the form $X_i > a$, where $a$ is some constant. In other words, the tests that are used on continuous data are of the form 'is feature i larger than the value a?'. \n",
    "* If the feature is categorical, the test will be $X_i = c$, where $c$ is one of the levels of the categorical variable. In other words, the tests that are used on categorical data are of the form 'is feature i of the same level as c?'. <p>\n",
    "    \n",
    "After assessing the test, this will result in two *children nodes*, one node being for all the data that satisfy the root node test and one node for all data points that do not satisfy the root node test.<p>\n",
    "    We then continue this process of finding informative rules and splitting the data. Resulting in a tree of nodes. The nodes in which we assign a value to the given observations are known as *leaf nodes*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Avoiding overfitting - pruning the tree\n",
    "If we allow the process of testing and splitting to continue indefinitely, we will have a tree with every leaf node being *pure*. *Pure* means that there are only data points of a single class label in the final leaf node. More often than not, such a tree will be very complex and highly overfitted to the training data. There are two common methods to prevent overfitting:\n",
    "1. **Pre-pruning**: Stopping overfitting before the creation of the tree. Common criteria for pre-pruning include limiting the maximum depth of the tree, limiting the maximum number of leaves, or placing a minimum size constraint on the nodes that must be satisfied for a split to occur.\n",
    "2. **Post-pruning**: Removing overfitted leaf nodes after the creation of the tree. This is commonly referred to as \"pruning\". <p>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Create a decision tree with no pruning\n",
    "\n",
    "We will create a decision tree with no pruning. We will see that some of the nodes will have only 1 sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_1 = DecisionTreeClassifier(random_state=0)\n",
    "tree_1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy on training set: {:.3f}\".format(tree_1.score(X_train, y_train))) \n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree_1.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cancer_labels = ['M', 'B']\n",
    "y_pred = tree_1.predict(X_test)\n",
    "cm = confusion_matrix(y_true = y_test, y_pred = y_pred, labels=cancer_labels)\n",
    "print(cm)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next lines of code might not work if you don't have graphviz installed in your computer:\n",
    "\n",
    "For Windows:\n",
    "\n",
    "1. Install windows package from: https://graphviz.gitlab.io/_pages/Download/Download_windows.html\n",
    "2. Install python graphviz package \n",
    "    !pip install graphviz \n",
    "    or\n",
    "    !conda install graphviz\n",
    "3. Add C:\\Program Files (x86)\\Graphviz2.38\\bin to User path\n",
    "4. Add C:\\Program Files (x86)\\Graphviz2.38\\bin\\dot.exe to System Path\n",
    "\n",
    "For Mac:\n",
    "\n",
    "1. MacPorts* provides both stable and development versions of Graphviz and the Mac GUI Graphviz.app. These can be obtained via the ports “graphviz”, “graphviz-devel”, “graphviz-gui” and “graphviz-gui-devel”: https://www.macports.org/\n",
    "2. Homebrew* has a Graphviz port: https://brew.sh/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(tree_1, out_file=\"tree.dot\", class_names=[\"malignant\", \"benign\"], feature_names=X.columns, impurity=False, filled=True)\n",
    "\n",
    "\n",
    "import graphviz\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
    "with open(\"tree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "graphviz.Source(dot_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Pre-Pruning a Tree\n",
    "Decision trees in scikit-learn are implemented in the DecisionTreeRegressor and DecisionTreeClassifier classes. scikit-learn only implements pre-pruning, not post-pruning, so we will only demonstrate how pre-pruning works.<p>\n",
    "    Now let’s apply pre-pruning to the tree, which will stop growing the tree before we perfectly fit it to the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">**Start Activity 3**</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 3: Set the maximum depth equal to 3, meaning only 3 consecutive splits can be made </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Python Code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">**End Activity 3**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(tree_2.score(X_train, y_train))) \n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree_2.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cancer_labels = ['M', 'B']\n",
    "y_pred = tree_2.predict(X_test)\n",
    "cm = confusion_matrix(y_true = y_test, y_pred = y_pred, labels=cancer_labels)\n",
    "print(cm)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(tree_2, out_file=\"tree.dot\", class_names=[\"malignant\", \"benign\"], feature_names=X.columns, impurity=False, filled=True)\n",
    "\n",
    "\n",
    "import graphviz\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
    "with open(\"tree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "graphviz.Source(dot_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualisation of the tree allows for an intuitive interpretation on how the algorithm classifies its data.<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">**Start Activity 4**</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 4: Did we improve our accuracy / precision-recall? Why? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Write your answer here:</b>\n",
    "#####################################################################################################################\n",
    "\n",
    "(Double-click here)\n",
    "\n",
    "\n",
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">**End Activity 4**</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2. Feature Importance\n",
    "There are some useful properties that we can derive to summarise the workings of the tree. A common example is *feature importance*, which as its name suggests, numericaly rates the importance each feature plays in the decision making process of the tree. It is a number between 0 and 1, with the sum of all feature importance's equalling 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature importances:\\n{}\".format(tree_2.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(model):\n",
    "    n_features = X.shape[1]\n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center') \n",
    "    plt.yticks(np.arange(n_features), X.columns) \n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_feature_importances(tree_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not an effective visualisation, as we have so many features. The solution is to remove all features that are of very low importance. We will only select the features whose feature importance is greater than 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(model):\n",
    "    \n",
    "    #locate indices of the features with feature importance greater than 0.01\n",
    "    indices = np.where(model.feature_importances_ > 0.01)[0]\n",
    "    \n",
    "    #extract the number of features that have non-zero feature importance\n",
    "    n_features = X.iloc[:,indices].shape[1]\n",
    "    \n",
    "    #plot the features that have a non-zero feature importance\n",
    "    plt.barh(range(n_features), model.feature_importances_[indices], align='center') \n",
    "    plt.yticks(np.arange(n_features), X.iloc[:,indices].columns) \n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances(tree_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the IMLP textbook: *\"... if a feature has a low feature_importance, it doesn’t mean that this feature is uninformative. It only means that the feature was not picked by the tree, likely because another feature encodes the same information.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Advantages/Disadvantages of Decision Trees\n",
    "**Advantages**:\n",
    "* Easy interpretation and visualisation of decision rules. Particularly to non-experts.\n",
    "* Very fast to train, and then predict.\n",
    "* Invariant to scaling of the data. This removes the need from preprocessing such as the standardisation that was needed for the regularized logistic models.\n",
    "* Are able to predict non-linear data.\n",
    "* Can be used to determine feature importance.\n",
    "* Further, provides automatic feature selection by only choosing the important features by which the data are split. This further reduces the need for preprocessing.\n",
    "* Provides probability estimates. <p>\n",
    "\n",
    "**Disadvantages**:\n",
    "* Tendency to overfit, even after pruning methods.\n",
    "* Often outperformed by other models, including the ensemble methods utilising the basic decision tree, which we will discuss now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Gradient Boosted Descision Trees\n",
    "Gradient Boosted Descision Trees are an *ensemble* of decision trees. *Ensemble* is a general term referring to methods that combine multiple machine learning models to create a more powerful model. There are two widely used ensembles based on decision trees: *Gradient Boosted Decision Trees*, and *Random Forests*. Here, we will introduce Gradient Boosted Decision Trees, and deal with Random Forests during this chapter's assessment.<p>\n",
    "\n",
    "Gradient boosting works by building a large number of trees where each tree tries to correct the mistakes of the previous one. The way this is achieved is by fitting each subsequent tree on a modified version of the original dataset, depending on how the previous trees performed. Given the current model, we fit a decision tree to the residuals (the unexplained variation) from the model. That is, we fit a new tree using the current residuals, rather than the whole response Y, as the response. We then add this new decision tree into the fitted function in order to update the residuals.<p> \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_binary = [0 if x =='M' else 1 for x in y_train]\n",
    "\n",
    "# Sanity Check\n",
    "print('Readmission (original y_train): ', y_train[35:45].reshape(1,-1))\n",
    "print('y_train after binary conversion: ', y_train_binary[35:45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_binary = [0 if x =='M' else 1 for x in y_test]\n",
    "\n",
    "# Sanity Check\n",
    "print('Readmission (original y_train): ', y_test[35:45].reshape(1,-1))\n",
    "print('y_train after binary conversion: ', y_test_binary[35:45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GradientBoostingClassifier(random_state=0)\n",
    "gbt.fit(X_train, y_train_binary)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gbt.score(X_train, y_train_binary)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(gbt.score(X_test, y_test_binary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gbt.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_true = y_test_binary, y_pred = y_pred)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Parameters\n",
    "In terms of parameters, we have the previous ones such as depth of the tree, maximum number of leaves, and minimum splitting size. However, strangely there is **no parameter for 'class_weight'** in gradient boosting trees for sklearn. This could be quite an issue for us, as class_weight has been one of the most important parameters for previous models. One way to get around this is to use an over/under sampling method, such as the SMOTE algorithm. Recall from chapter 3 that SMOTE generates synthetic data points from the *minority* class. <p> \n",
    "    We also have two new parameters:\n",
    "* n_estimators, the number of trees in the ensemble. Increasing n_estimators in gradient boosting leads to a more complex model, which may lead to overfitting\n",
    "* Learning_rate. The learning rate controls how strongly each tree tries to correct the mistakes of the previous trees. Learning_rate is a decimal number between 0 and 1, with low values indicating slow learning and higher values indicating fast learning.\n",
    "\n",
    "Ideally, we would like to tune the three most important parameters: n_estimator, max_depth, and learning_rate. However, as we are restrained by both time and computational power, we will restrict ourselves to tuning only the learning_rate. For max_depth, it is standard practice to limit the ensemble to quite shallow trees, so we we will choose max_depth = 3. The parameters n_estimator and learning_rate are highly interconnected, as a lower learning_rate means that more trees are needed to build a model of similar complexity. Using this knowledge, we will choose a value of n_estimators that is feasible given our time and computational power constraints, and then find the best learning_rate given this value of n_estimators.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Grid Search\n",
    "Here, we will attempt to find the parameters that maximise the f1 score, macro averaged. Remember, when using cross validation and grid search it is good practice to reserve the test set until **after** we finish selecting parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GradientBoostingClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1. First Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [100],\n",
    "              'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now initialise the GridSearchCV class by passing it the gradient boosted tree we have created, *gbt*, our paramater grid, *param_grid*, and specifying how many folds we would like. We must consider the computational complexity of the algorithm, so we can't set cv too high. We choose 5 folds. We also specify scoring = 'f1_macro' to designate that we would like to use the F1 measure with an unweighted average of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_search = GridSearchCV(gbt, param_grid=param_grid, cv=5, scoring = 'f1_macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation average f1 score: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2. Second Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">**Start Activity 5**</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 5a: Define a second search with different values close to 0.2 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Python Code Here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 5b: Define the variable grid_search in which we will do \"GridSearchCV\" with CV=5 and for the f1_macro score </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Python Code Here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 5c: \"Fit\" the models </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Python Code Here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 5d: Print the best parameters </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Python Code Here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">**End Activity 5**</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Fit and evaluate the gradient boosted tree with our optimal parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">**Start Activity 6**</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 6a: Train the new classifier with the best parameters found above </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Python Code Here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 6b: Compute the confusion matrix </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Python Code Here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">**End Activity 5**</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. Visualising the Gradient Boosted Tree: Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature importances:\\n{}\".format(gbt.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(model):\n",
    "    n_features = X.shape[1]\n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center') \n",
    "    plt.yticks(np.arange(n_features), X.columns) \n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_feature_importances(gbt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not an effective visualisation, as we have so many features. The solution is to remove all features that are of very low importance. We will only plot the features with importance of at least 0.02.<p>\n",
    "    Notice that almost all the features have a non-zero importance. This is in contrast to the regular decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(model):\n",
    "    \n",
    "    #locate indices of the features with non-zero feature importance\n",
    "    indices = np.where(model.feature_importances_ >= 0.02)[0]\n",
    "    \n",
    "    #extract the number of features that have non-zero feature importance\n",
    "    n_features = X.iloc[:,indices].shape[1]\n",
    "    \n",
    "    #plot the features that have a non-zero feature importance\n",
    "    plt.barh(range(n_features), model.feature_importances_[indices], align='center') \n",
    "    plt.yticks(np.arange(n_features), X.iloc[:,indices].columns) \n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances(gbt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, the most important feature is num_lab_procedures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7. Advantages/Disadvantages of Gradient Boosted Trees\n",
    "**Advantages**:\n",
    "* Can be very powerful, provided the parameters are tuned correctly\n",
    "* Build trees one at a time, where each new tree helps to correct errors made by previously trained trees\n",
    "\n",
    "**Disadvantages**:\n",
    "* Requires careful tuning of the parameters\n",
    "* Longer time to train, because trees are built sequentially\n",
    "* Long time to predict\n",
    "* Can be susceptible to overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
