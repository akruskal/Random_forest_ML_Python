{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/HDAT9500Banner.PNG)\n",
    "<br>\n",
    "\n",
    "# Chapter 4: Tree Based Methods\n",
    "\n",
    "# Demostration: Random Forest\n",
    "\n",
    "\n",
    "# 1. Introduction\n",
    "\n",
    "In this exercise, we will introduce Random Forest and run an example.\n",
    "\n",
    "\n",
    "## 1.1. Aims of the Exercise:\n",
    " 1. To demonstrate how Random Forest Trees are build, following an example of Book 1.\n",
    "\n",
    " \n",
    "It aligns with all the learning outcome of our course: \n",
    "\n",
    "1.\tDistinguish a range of task specific machine learning techniques appropriate for Health Data Science.\n",
    "2.\tDesign machine learning tasks for Health Data Science scenarios.\n",
    "3.\tConstruct appropriate training and test sets for health research data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests, like Gradient Boosted Decision Trees, are ensembles of decision trees. Random forests address the issue of overfitting present in decision trees. \n",
    "> *A random forest is essentially a collection of decision trees, where each tree is slightly different from the others. The idea behind random forests is that each tree might do a relatively good job of predicting, but will likely overfit on part of the data. If we build many trees, all of which work well and overfit in different ways, we can reduce the amount of overfitting by averaging their results.*\n",
    ">\n",
    "> Introduction to Machine Learning with Python, page 83.\n",
    "\n",
    "The way to ensure that the trees are slightly different is to introduce some randomness into the model. This is also where Random Forests derive their name. There are two methods of introducing randomness. For each tree, we:\n",
    "* Randomly sample the data points used to train the tree. This is achieved by taking a *bootstrap sample* of the data.\n",
    "* Randomly select a subset of the total feature space used to train the tree. Rather than using the whole feature space to find a test for each node, **at each node** we randomly select a subset of the features, and look for the best test involving one of these sampled features. The number of features that are selected is controlled by the max_features parameter. Please note that this selection of a subset of features is repeated separately in each node, so that each node *within a tree* can make a decision using a different subset of the features.\n",
    "\n",
    "The bootstrap sampling leads to each decision tree in the random forest being built on a slightly different dataset. Because of the selection of features in each node, each split in each tree operates on a different subset of features. Together, these two randomisations ensure that all the trees in the random forest are different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Exemplifying example\n",
    "Here, we replicate the example from the text book, as a quick way to visualise how random forests work. We have to use a dummy data set like this, as it would be very hard to do this for our hospital data, given that the number of features is so large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import mglearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "X_example, y_example = make_moons(n_samples=100, noise=0.25, random_state=0)\n",
    "X_train_example, X_test, y_train_example, y_test_example = train_test_split(X_example, y_example, stratify=y_example, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the random forest.\n",
    "forest = RandomForestClassifier(n_estimators=5, random_state=0)\n",
    "forest.fit(X_train_example, y_train_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the individual trees and the random forest.\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 10))\n",
    "\n",
    "for i, (ax, tree) in enumerate(zip(axes.ravel(), forest.estimators_)):\n",
    "    ax.set_title(\"Tree {}\".format(i))\n",
    "    mglearn.plots.plot_tree_partition(X_train_example, y_train_example, tree, ax=ax)\n",
    "\n",
    "mglearn.plots.plot_2d_separator(forest, X_train_example, fill=True, ax=axes[-1, -1], alpha=.4)\n",
    "axes[-1, -1].set_title(\"Random Forest\")\n",
    "mglearn.discrete_scatter(X_train_example[:, 0], X_train_example[:, 1], y_train_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see that each tree overfits and learns the data in different ways. When we aggregate the trees, we form a much more robust classifier(see last plot).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
