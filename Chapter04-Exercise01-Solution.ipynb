{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/HDAT9500Banner.PNG)\n",
    "<br>\n",
    "\n",
    "# Chapter 4: Tree Based Methods\n",
    "# Exercise 01: Decision Trees and Gradient Boosting Trees\n",
    "\n",
    "\n",
    "# 1. Introduction\n",
    "\n",
    "In this exercise, we will introduce tree based methods. First, we will learn about the basic decision tree, then we will see how decision tree performance can be improved via ensemble methods - specifically, gradient descent boosting.\n",
    "\n",
    "\n",
    "## 1.1. Aims of the Exercise:\n",
    " 1. To introduce the single Decision Tree, as well as the Gradient Boosted Trees.\n",
    " 2. To explore parameters and determine appropriate choices.\n",
    "\n",
    " \n",
    "It aligns with all of the learning outcomes of our course: \n",
    "\n",
    "1.\tDistinguish a range of task specific machine learning techniques appropriate for Health Data Science.\n",
    "2.\tDesign machine learning tasks for Health Data Science scenarios.\n",
    "3.\tConstruct appropriate training and test sets for health research data.\n",
    "\n",
    "\n",
    "## 1.2. Jupyter Notebook Intructions\n",
    "1. Read the content of each cell.\n",
    "2. Where necessary, follow the instructions that are written in each cell.\n",
    "3. Run/Execute all the cells that contain Python code sequentially (one at a time), using the \"Run\" button.\n",
    "4. For those cells in which you are asked to write some code, please write the Python code first and then execute/run the cell.\n",
    " \n",
    "## 1.3. Tips\n",
    " 1. The square brackets on the left hand side of each cell indicate whether the cell has been executed or not. Empty square brackets mean that the cell has not been executed, whereas square brackets that contain a number means that the cell has been executed. Run all the cells in sequence, using the \"Run\" button.\n",
    " 2. To edit this notebook, just double-click in each cell. In the document, each cell can be a \"Code\" cell or \"text-Markdown\" cell. To choose between these two options, go to the combo-box above. \n",
    " 3. If you want to save your notebook, please make sure you press the \"floppy disk\" icon button above. \n",
    " 4. To clean the content of all cells and re-start the Notebook, please go to Cell->All Output->Clear\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load the Wisconsin Cancer Data Set\n",
    "\n",
    "For data dictionary and all information:\n",
    "https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 10:22:32) [MSC v.1900 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from plotnine import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = pd.read_csv('data/breast-cancer-wisconsin-data/data.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "            ...             radius_worst  texture_worst  perimeter_worst  \\\n",
       "0           ...                    25.38          17.33           184.60   \n",
       "1           ...                    24.99          23.41           158.80   \n",
       "2           ...                    23.57          25.53           152.50   \n",
       "3           ...                    14.91          26.50            98.87   \n",
       "4           ...                    22.54          16.67           152.20   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 32)\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check:\n",
    "display(cancer[:][:5])\n",
    "print(cancer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                           int64\n",
       "diagnosis                   object\n",
       "radius_mean                float64\n",
       "texture_mean               float64\n",
       "perimeter_mean             float64\n",
       "area_mean                  float64\n",
       "smoothness_mean            float64\n",
       "compactness_mean           float64\n",
       "concavity_mean             float64\n",
       "concave points_mean        float64\n",
       "symmetry_mean              float64\n",
       "fractal_dimension_mean     float64\n",
       "radius_se                  float64\n",
       "texture_se                 float64\n",
       "perimeter_se               float64\n",
       "area_se                    float64\n",
       "smoothness_se              float64\n",
       "compactness_se             float64\n",
       "concavity_se               float64\n",
       "concave points_se          float64\n",
       "symmetry_se                float64\n",
       "fractal_dimension_se       float64\n",
       "radius_worst               float64\n",
       "texture_worst              float64\n",
       "perimeter_worst            float64\n",
       "area_worst                 float64\n",
       "smoothness_worst           float64\n",
       "compactness_worst          float64\n",
       "concavity_worst            float64\n",
       "concave points_worst       float64\n",
       "symmetry_worst             float64\n",
       "fractal_dimension_worst    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.Split the data into features and response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cancer.drop(axis=1, columns=['id', 'diagnosis'])\n",
    "y = cancer[['diagnosis']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "# display(X[:][:5])\n",
    "# display(y[:][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Split the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Decision Trees\n",
    "Decision Trees involve segmenting the feature space (the space of our predictor variables) into a number of different regions. The method can be used for both regression (predicting numeric response variable) and classification (classifying a categorical response variable). As the set of splitting rules used to segment the feature space can be summarized into a hierarchy of if/else statements in the form of a tree, these types of approaches are known as decision tree methods.<p>\n",
    "    In the case of **regression**, in order to make a prediction for any particular observation, we usually use the mean of the training observations in the region to which it belongs. For **classification**, we usually use the mode of the training observations in the region to which the data point belongs. Recall that the mode is the most frequent element. So, *for classification we predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs*.<p>\n",
    "        We will use tree methods to predict cancer diagnosis, which is a classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Growing a simple decision tree\n",
    "Beginning with what is known as the *root node*, the node containing the entire dataset, we split the data based on the feature that provides the most information about the response variable. The split is achieved by using a *test*. The form of the test depends on the data type of the chosen feature.\n",
    "* If the feature of choice is continuous, the test will be of the form $X_i > a$, where $a$ is some constant. In other words, the tests that are used on continuous data are of the form 'is feature i larger than the value a?'. \n",
    "* If the feature is categorical, the test will be $X_i = c$, where $c$ is one of the levels of the categorical variable. In other words, the tests that are used on categorical data are of the form 'is feature i of the same level as c?'. <p>\n",
    "    \n",
    "After assessing the test, the result will be two *children nodes*, one node being for all the data that satisfy the root node test and one node for all data points that do not satisfy the root node test.<p>\n",
    "    We then continue this process of finding informative rules and splitting the data. Resulting in a tree of nodes. The nodes in which we assign a value to the given observations are known as *leaf nodes*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Avoiding overfitting - pruning the tree\n",
    "If we allow the process of testing and splitting to continue indefinitely, we will have a tree with every leaf node being *pure*. *Pure* means that there are only data points of a single class label in the final leaf node. More often than not, such a tree will be very complex and highly overfitted to the training data. There are two common methods to prevent overfitting:\n",
    "1. **Pre-pruning**: Preventing overfitting before the creation of the tree. Common criteria for pre-pruning includes limiting the maximum depth of the tree, limiting the maximum number of leaves, or placing a minimum size constraint on the nodes that must be satisfied for a split to occur.\n",
    "2. **Post-pruning**: Removing overfitted leaf nodes after the creation of the tree. This is commonly referred to as \"pruning\". <p>\n",
    "    \n",
    "Decision trees in scikit-learn are implemented in the **DecisionTreeRegressor** and **DecisionTreeClassifier** classes. **scikit-learn** only implements pre-pruning, not post-pruning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Create a decision tree with no pruning\n",
    "\n",
    "We will create a decision tree with no pruning. We will see that some of the nodes will have only 1 sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_decision_tree = DecisionTreeClassifier(random_state=0)\n",
    "simple_decision_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 1.000\n",
      "Accuracy on test set: 0.930\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on training set: {:.3f}\".format(simple_decision_tree.score(X_train, y_train))) \n",
    "print(\"Accuracy on test set: {:.3f}\".format(simple_decision_tree.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/Confusion_matrix.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[41  1]\n",
      " [ 7 65]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x243c3918908>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cancer_labels = ['M', 'B']\n",
    "y_pred = simple_decision_tree.predict(X_test)\n",
    "cm = confusion_matrix(y_true = y_test, y_pred = y_pred, labels=cancer_labels)\n",
    "print(cm)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + cancer_labels)\n",
    "ax.set_yticklabels([''] + cancer_labels)\n",
    "plt.xlabel('Predicted class')\n",
    "plt.ylabel('True class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          B       0.98      0.90      0.94        72\n",
      "          M       0.85      0.98      0.91        42\n",
      "\n",
      "avg / total       0.94      0.93      0.93       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next lines of code might not work if you don't have graphviz installed in your computer:\n",
    "\n",
    "For Windows:\n",
    "\n",
    "1. Install windows package from: https://graphviz.gitlab.io/_pages/Download/Download_windows.html\n",
    "2. Install python graphviz package \n",
    "    !pip install graphviz \n",
    "    or\n",
    "    !conda install graphviz\n",
    "3. Add C:\\Program Files (x86)\\Graphviz2.38\\bin to User path\n",
    "4. Add C:\\Program Files (x86)\\Graphviz2.38\\bin\\dot.exe to System Path\n",
    "\n",
    "For Mac:\n",
    "\n",
    "1. MacPorts* provides both stable and development versions of Graphviz and the Mac GUI Graphviz.app. These can be obtained via the ports “graphviz”, “graphviz-devel”, “graphviz-gui” and “graphviz-gui-devel”: https://www.macports.org/\n",
    "2. Homebrew* has a Graphviz port: https://brew.sh/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-4acb9875ec37>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgraphviz\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"PATH\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpathsep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'C:/Program Files (x86)/Graphviz2.38/bin/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'graphviz'"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(simple_decision_tree, out_file=\"tree.dot\", class_names=[\"malignant\", \"benign\"], feature_names=X.columns, impurity=False, filled=True)\n",
    "\n",
    "\n",
    "import graphviz\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
    "with open(\"tree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "graphviz.Source(dot_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Pre-pruning a tree\n",
    "As mentioned previously, scikit-learn only implements pre-pruning, not post-pruning, so we will only demonstrate how pre-pruning works.<p>\n",
    "    Now let’s apply pre-pruning to the tree, which will stop growing the tree before we perfectly fit it to the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">**Start Activity**</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 1: Set the maximum depth equal to 3, meaning only 3 consecutive splits can be made </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Python Code here\n",
    "decision_tree_max_depth3 = DecisionTreeClassifier(random_state=0, max_depth = 3)\n",
    "decision_tree_max_depth3.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(decision_tree_max_depth3.score(X_train, y_train))) \n",
    "print(\"Accuracy on test set: {:.3f}\".format(decision_tree_max_depth3.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">**End Activity**</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1. Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/Confusion_matrix.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cancer_labels = ['M', 'B']\n",
    "y_pred_max_depth3 = decision_tree_max_depth3.predict(X_test)\n",
    "cm_max_depth3 = confusion_matrix(y_true = y_test, y_pred = y_pred_max_depth3, labels=cancer_labels)\n",
    "print(cm_max_depth3)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + cancer_labels)\n",
    "ax.set_yticklabels([''] + cancer_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred_max_depth3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(decision_tree_max_depth3, out_file=\"tree.dot\", class_names=[\"malignant\", \"benign\"], feature_names=X.columns, impurity=False, filled=True)\n",
    "\n",
    "\n",
    "import graphviz\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
    "with open(\"tree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "graphviz.Source(dot_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization of the tree allows for an intuitive interpretation on how the algorithm classifies its data.<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">**Start Activity 4**</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 2: Did we improve our accuracy / precision-recall? Why? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Write your answer here:</b>\n",
    "#####################################################################################################################\n",
    "\n",
    "We did improve our accuracy, precision and recall.\n",
    "Why? Because by restricting the depth of the 3, we avoid overfitting. That is, the model doesn't overfit to the training set. It generalizes better. \n",
    "\n",
    "\n",
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">**End Activity 4**</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2. Feature importance\n",
    "There are some useful properties that we can derive to summarise the workings of the tree. A common example is *feature importance*, which as its name suggests, numerically rates the importance each feature plays in the decision making process of the tree. It is a number between 0 and 1, with the sum of all feature importances equalling to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature importances:\\n{}\".format(decision_tree_max_depth3.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(model):\n",
    "    n_features = X.shape[1]\n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center') \n",
    "    plt.yticks(np.arange(n_features), X.columns) \n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_feature_importances(decision_tree_max_depth3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not an effective visualization, as we have so many features. The solution is to remove all features that are of very low importance. We will only select the features whose feature importance is greater than 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(model):\n",
    "    \n",
    "    #locate indices of the features with feature importance greater than 0.01\n",
    "    indices = np.where(model.feature_importances_ > 0.01)[0]\n",
    "    \n",
    "    #extract the number of features that have non-zero feature importance\n",
    "    n_features = X.iloc[:,indices].shape[1]\n",
    "    \n",
    "    #plot the features that have a non-zero feature importance\n",
    "    plt.barh(range(n_features), model.feature_importances_[indices], align='center') \n",
    "    plt.yticks(np.arange(n_features), X.iloc[:,indices].columns) \n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances(decision_tree_max_depth3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the IMLP textbook: **\"... if a feature has a low feature_importance, it doesn’t mean that this feature is uninformative. It only means that the feature was not picked by the tree, likely because another feature encodes the same information.\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Advantages/disadvantages of decision trees\n",
    "**Advantages**:\n",
    "* Easy interpretation and visualization of decision rules. Particularly to non-experts.\n",
    "* Very fast to train, and then predict.\n",
    "* Invariant to scaling of the data. This removes the need for preprocessing such as the standardisation that was needed for the regularized logistic models.\n",
    "* Are able to predict non-linear data.\n",
    "* Can be used to determine feature importance.\n",
    "* Further, provides automatic feature selection by only choosing the important features by which the data are split. This further reduces the need for preprocessing.\n",
    "* Provides probability estimates. <p>\n",
    "\n",
    "**Disadvantages**:\n",
    "* Tendency to overfit, even after pruning methods.\n",
    "* Often outperformed by other models, including the ensemble methods utilising the basic decision tree, which we will discuss now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Gradient Boosted Decision Trees\n",
    "Gradient Boosted Descision Trees are an *ensemble* of decision trees. *Ensemble* is a general term referring to methods that combine multiple machine learning models to create a more powerful model. There are two widely used ensembles based on decision trees: *Gradient Boosted Decision Trees*, and *Random Forests*. Here, we will introduce Gradient Boosted Decision Trees, and deal with Random Forests during this chapter's assessment.<p>\n",
    "\n",
    "Gradient boosting works by building a large number of trees where each tree tries to correct the mistakes of the previous one. The way this is achieved is by fitting each subsequent tree on a modified version of the original dataset, depending on how the previous trees performed. Given the current model, we fit a decision tree to the residuals (the unexplained variation) from the model. That is, we fit a new tree using the current residuals, rather than the whole response Y, as the response. We then add this new decision tree into the fitted function in order to update the residuals.<p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Readings and videos:\n",
    "1. Scikit-learn API: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "2. Video 1: Regression https://www.youtube.com/watch?v=3CC4N4z3GJc\n",
    "3. Video 2: Classification https://www.youtube.com/watch?v=jxuNLH5dXCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_binary = [0 if x =='M' else 1 for x in y_train]\n",
    "\n",
    "# Sanity Check\n",
    "print('Cancer (original y_train): ', y_train[35:45].reshape(1,-1))\n",
    "print('y_train after binary conversion: ', y_train_binary[35:45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_binary = [0 if x =='M' else 1 for x in y_test]\n",
    "\n",
    "# Sanity Check\n",
    "print('Cancer (original y_train): ', y_test[35:45].reshape(1,-1))\n",
    "print('y_train after binary conversion: ', y_test_binary[35:45])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create the 'gbt' object *that encapsulates the algorithm that will be used to build the model from\n",
    "the training data, as well the algorithm to make predictions on new data points. It will\n",
    "also hold the information that the algorithm has extracted from the training data* (source: Book 1, Chapter 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GradientBoostingClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*To build the model on the training set, we call the fit method of the knn object* (source: Book 1, Chapter 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt.fit(X_train, y_train_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The fit method returns the knn object itself (and modifies it in place), so we get a\n",
    "string representation of our classifier. The representation shows us which parameters\n",
    "were used in creating the model. Nearly all of them are the default values, but you can\n",
    "also find* random_state=0, *which is the* hyper-parameter *that we passed. Most models in\n",
    "scikit-learn have many parameters, but the majority of them are either speed optimizations\n",
    "or for very special use cases.* (source: Book1, Chapter 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate accuracy for the training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy on training set: {:.3f}\".format(gbt.score(X_train, y_train_binary)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(gbt.score(X_test, y_test_binary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_gbt = gbt.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print('Confusion matrix:')\n",
    "print('')\n",
    "cm_gbt = confusion_matrix(y_true = y_test_binary, y_pred = y_pred_gbt)\n",
    "print(cm_gbt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of the confusion matrix:\n",
    "\n",
    "![alt text](images/Confusion_matrix.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred_max_depth3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Parameters\n",
    "In terms of parameters, we have the previous ones such as **depth of the tree**, **maximum number of leaves**, and **minimum splitting size**. However,  there is **no parameter for 'class_weight'** in gradient boosting trees for sklearn.  <p> \n",
    "    We also have two new parameters:\n",
    "* n_estimators, the number of trees in the ensemble. Increasing n_estimators in gradient boosting leads to a more complex model, which may lead to overfitting\n",
    "* Learning_rate. The learning rate controls how strongly each tree tries to correct the mistakes of the previous trees. Learning_rate is a decimal number between 0 and 1, with low values indicating slow learning and higher values indicating fast learning.\n",
    "\n",
    "Ideally, we would like to tune the three most important parameters: n_estimator, max_depth, and learning_rate. However, as we are restrained by both time and computational power, we will restrict ourselves to tuning only the learning_rate. For max_depth, it is standard practice to limit the ensemble to quite shallow trees, so we we will choose max_depth = 3. The parameters n_estimator and learning_rate are highly interconnected, as a lower learning_rate means that more trees are needed to build a model of similar complexity. Using this knowledge, we will choose a value of n_estimators that is feasible given our time and computational power constraints, and then find the best learning_rate given this value of n_estimators.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Grid search\n",
    "Here, we will attempt to find the parameters that maximise the f1 score, macro averaged. Remember, when using cross-validation and grid search it is good practice to reserve the test set until **after** we finish selecting parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_gridsearch = GradientBoostingClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1. First search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [15, 25, 50, 100],\n",
    "              'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3, 0.4]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now initialise the GridSearchCV class by passing it the gradient boosted tree we have created, *gbt*, our paramater grid, *param_grid*, and specifying how many folds we would like. We must consider the computational complexity of the algorithm, so we can't set cv too high. We choose 5 folds. We also specify scoring = 'f1' to designate that we would like to use the F1 measure with an unweighted average of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_search = GridSearchCV(gbt_gridsearch, param_grid=param_grid, cv=5, scoring = 'f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the grid search. *Fitting the GridSearchCV object not only searches for the best parameters, but also\n",
    "automatically fits a new model on the whole training dataset with the parameters that\n",
    "yielded the best cross-validation performance* (source: Book 1, Chapter 5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The important thing here is that we did not use the test set to choose the parameters. The parameters that were found are scored in the best_params_ attribute, and the best cross-validation accuracy (the mean accuracy\n",
    "over the different splits for this parameter setting) is stored in best_score_:* (source: Book 1, Chapter 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation average f1 score: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Again, be careful not to confuse best_score_ with the generalization\n",
    "performance of the model as computed by the score method\n",
    "on the test set. Using the score method (or evaluating the output of\n",
    "the predict method) employs a model trained on the whole training\n",
    "set. The best_score_ attribute stores the mean cross-validation\n",
    "accuracy, with cross-validation performed on the training set.* (source: Book 1, Chapter 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedure \".score()\" will display F1 score of the positive class as we changed the \"scoring='f1'\" in the GridSearchCV function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test set F1: {:.2f}\".format(grid_search.score(X_test, y_test_binary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, in order to print accuracy,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's predict in the training set, in order to calculate accuracy in the train set too, to see if our model is overfitted.\n",
    "y_pred_grid_search_training_set= grid_search.predict(X_train)\n",
    "\n",
    "# We predict in the test set\n",
    "y_pred_grid_search = grid_search.predict(X_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy on training set: {:.2f}\".format(accuracy_score(y_train_binary, y_pred_grid_search_training_set)))\n",
    "print(\"Accuracy on test set: {:.2f}\".format(accuracy_score(y_test_binary, y_pred_grid_search)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_grid_search = confusion_matrix(y_true = y_test_binary, y_pred = y_pred_grid_search)\n",
    "print(cm_grid_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of the confusion matrix:\n",
    "\n",
    "![alt text](images/Confusion_matrix.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test_binary, y_pred_grid_search))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2. Second search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">**Start Activity**</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 3: Define a second search with different values close to the optimal learning rate value found  in the previous section</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal value was 0.2. Thus, we define values in the vecinity of 0.2, just to test if we can find some learning rate that performs better. We set the number of estimators to 50, as it is the best value found before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Python Code Here:\n",
    "param_grid_in_detail = {'n_estimators': [50],\n",
    "              'learning_rate':  [0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 4: Define the variable grid_search in which we will do \"GridSearchCV\" with CV=5 and for the f1 score </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Python Code Here:\n",
    "grid_search_in_detail = GridSearchCV(gbt, param_grid=param_grid_in_detail, cv=5, scoring = 'f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 5: \"Fit\" the models </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Python Code Here:\n",
    "grid_search_in_detail.fit(X_train, y_train_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 6: Print the best parameters </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Python Code Here:\n",
    "print(\"Best parameters: {}\".format(grid_search_in_detail.best_params_))\n",
    "print(\"Best cross-validation f1 score: {:.2f}\".format(grid_search_in_detail.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">**End Activity**</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Fit and evaluate the gradient boosted tree with our optimal parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">**Start Activity**</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 7: Train the new classifier with the best parameters found above </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>**There is no need to train a new model**</font>\n",
    "\n",
    "From Book 1, Chapter 5 [27]\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "Fitting the GridSearchCV object not only searches for the best parameters, but also\n",
    "automatically fits a new model on the whole training dataset with the parameters that\n",
    "yielded the best cross-validation performance. What happens in fit is therefore\n",
    "equivalent to the result of the In[21] code we saw at the beginning of this section. The\n",
    "GridSearchCV class provides a very convenient interface to access the retrained\n",
    "model using the predict and score methods. To evaluate how well the best found\n",
    "parameters generalize, we can call score on the test set as follows:\n",
    "\n",
    "Bear in mind, that \"score\" will display F1 (and not accuracy) because we changed it in the GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Python Code Here:\n",
    "print(\"Test set F1: {:.2f}\".format(grid_search_in_detail.score(X_test, y_test_binary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 8: Compute the confusion matrix </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_grid_search_in_detail = grid_search.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Python Code Here:\n",
    "cm_after_grid_search_in_detail = confusion_matrix(y_true = y_test_binary, y_pred = y_pred_grid_search_in_detail)\n",
    "print(cm_after_grid_search_in_detail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of the confusion matrix:\n",
    "\n",
    "![alt text](images/Confusion_matrix.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the model didn't improve at all after trying to tune several hyper-parameters.\n",
    "Feel free to try new hyper-parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">**End Activity**</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. Visualising the gradient boosted tree: feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember: \n",
    "\n",
    "You can access the model with the\n",
    "best parameters trained on the whole training set using the best_estimator_.\n",
    "\n",
    "We know that the best model found by GridSearchCV, **trained on all the training data**, is stored in grid.best_estimator_:\n",
    "\n",
    "print(\"Best estimator:\\n{}\".format(grid.best_estimator_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature importances:\\n{}\".format(grid_search_in_detail.best_estimator_.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(model):\n",
    "    n_features = X.shape[1]\n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center') \n",
    "    plt.yticks(np.arange(n_features), X.columns) \n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_feature_importances(grid_search_in_detail.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not an effective visualization, as we have so many features. The solution is to remove all features that are of very low importance. We will only plot the features with importance of at least 0.02.<p>\n",
    "    Notice that almost all the features have a non-zero importance. This is in contrast to the regular decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(model):\n",
    "    \n",
    "    #locate indices of the features with non-zero feature importance\n",
    "    indices = np.where(model.feature_importances_ >= 0.02)[0]\n",
    "    \n",
    "    #extract the number of features that have non-zero feature importance\n",
    "    n_features = X.iloc[:,indices].shape[1]\n",
    "    \n",
    "    #plot the features that have a non-zero feature importance\n",
    "    plt.barh(range(n_features), model.feature_importances_[indices], align='center') \n",
    "    plt.yticks(np.arange(n_features), X.iloc[:,indices].columns) \n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances(grid_search_in_detail.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7. Advantages/disadvantages of gradient boosted trees\n",
    "**Advantages**:\n",
    "* Can be very powerful, provided the parameters are tuned correctly\n",
    "* Build trees one at a time, where each new tree helps to correct errors made by previously trained trees\n",
    "\n",
    "**Disadvantages**:\n",
    "* Requires careful tuning of the parameters\n",
    "* Longer time to train, because trees are built sequentially\n",
    "* Long time to predict\n",
    "* Can be susceptible to overfitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
