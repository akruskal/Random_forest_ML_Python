{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/HDAT9500Banner.PNG)\n",
    "<br>\n",
    "\n",
    "# Chapter 4: Tree Based Methods\n",
    "\n",
    "# Assessment: Random Forest\n",
    "\n",
    "\n",
    "# 1. Introduction\n",
    "\n",
    "In this exercise, we will introduce tree based methods. First, we will learn about the basic decision tree, then we will see how decision trees performance can be improved via ensemble methods - specifically, gradient descent boosting.\n",
    "\n",
    "\n",
    "## 1.1. Aims of the Exercise:\n",
    " 1. To introduce Random Forest.\n",
    " 2. To explore parameters and determine appropriate choices.\n",
    "\n",
    " \n",
    "It aligns with all the learning outcome of our course: \n",
    "\n",
    "1.\tDistinguish a range of task specific machine learning techniques appropriate for Health Data Science.\n",
    "2.\tDesign machine learning tasks for Health Data Science scenarios.\n",
    "3.\tConstruct appropriate training and test sets for health research data.\n",
    "\n",
    "\n",
    "## 1.2. Jupyter Notebook Intructions\n",
    "1. Read the content of each cell.\n",
    "2. Where necessary, follow the instructions that are written in each cell.\n",
    "3. Run/Execute all the cells that contain Python code sequentially (one at a time), using the \"Run\" button.\n",
    "4. For those cells in which you are asked to write some code, please write the Python code first and then execute/run the cell.\n",
    " \n",
    "## 1.3. Tips\n",
    " 1. The square brackets on the left hand side of each cell indicate whether the cell has been executed or not. Empty square brackets mean that the cell has not been executed, whereas square brackets that contain a number means that the cell has been executed. Run all the cells in sequence, using the \"Run\" button.\n",
    " 2. To edit this notebook, just double-click in each cell. In the document, each cell can be a \"Code\" cell or \"text-Markdown\" cell. To choose between these two options, go to the combo-box above. \n",
    " 3. If you want to save your notebook, please make sure you press \"the floppy disk\" icon button above. \n",
    " 4. To clean the content of all cells and re-start Notebook, please go to Cell->All Output->Clear\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load the liver patient dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patients with Liver disease have been continuously increasing because of excessive consumption of alcohol, inhale of harmful gases, etc. <p>\n",
    "\n",
    "This data set contains 416 liver patient records and 167 non liver patient records collected from North East of Andhra Pradesh, India. The \"Dataset\" column is a class label used to divide groups into liver patient (liver disease = 1) or not (no disease = 0). This data set contains 441 male patient records and 142 female patient records.<p>\n",
    "\n",
    "Any patient whose age exceeded 89 is listed as being of age \"90\".<p>\n",
    "\n",
    "Use these patient records to determine which patients have liver disease and which ones do not.<p>\n",
    "\n",
    "Attribute Information:<p>\n",
    "    1. Age: Age of the patient\n",
    "    2. Gender: Gender of the patient\n",
    "    3. Total_Bilirubin: Total Bilirubin\n",
    "    4. Direct_Bilirubin: Direct Bilirubin\n",
    "    5. Alkaline_Phosphatase: Alkaline Phosphatase\n",
    "    6. Alanine_Aminotransferase: Alanine Aminotransferase\n",
    "    7. Aspartate_Aminotransferase: Aspartate Aminotransferase\n",
    "    8. Total_Proteins: Total Proteins\n",
    "    9. Albumin: Albumin\n",
    "    10. Albumin_and_Globulin_Ratio: Albumin and Globulin Ratio\n",
    "    \n",
    "Source: https://www.kaggle.com/arslanengr/liver-patient-classification-data-analysis/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "#For this notebook to work, Python must be 3.6.4 or 3.6.5\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from plotnine import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liver = pd.read_csv('data/liver-data/data_Clean.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check:\n",
    "display(liver[:][:5])\n",
    "print(liver.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.Split the data into features and response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = liver.drop(['liver_patient'], axis = 1)\n",
    "y = liver[['liver_patient']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X[:][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(y[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Predicting Liver Patient using Random forests\n",
    "In this section we will tune the parameters and eventually build a random forest to predict if a patient suffers from liver disease or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Parameters\n",
    "Here are some of the more important parameters:\n",
    "\n",
    "* n_estimators: Unlike Gradient Boosted Decision Trees, Random Forests *always* improve with an increased number of estimators, and there is no danger of overfitting. However, there are diminishing returns, with improvement quickly plateauing. The number of estimators is limited by our time and computational resources. <p>\n",
    "* max_features: This is the number of features to consider when looking for the best split. max_features determines how random each tree is. A large max_features means the trees will be more similar, possibly allowing for overfitting. On the other hand, a smaller max_features reduces overfitting, but may force each tree to be very deep. For classification, it is common to use the default of max_features = $\\sqrt {number\\:of\\:features}$.<p>\n",
    "    \n",
    "* class_weight: Random Forest has a parameter to penalise incorrect class labels differently. This is very useful for our imbalanced data.<p>\n",
    "    \n",
    "* max_depth: This parameter controls the maximum depth of the tree. If not specified, then nodes are expanded until all leaves are pure, or until all leaves contain less than min_samples_split samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Tuning our Model\n",
    "We will use a cross validated grid search, using GridSearchCV. The parameter we will tune is max_depth. We will set n_estimators = 100, and use the default max_features = $\\sqrt {number\\:of\\:features}$. <p>\n",
    "\n",
    "We will seek to maximise the unweighted average of f1 score (f1_macro). We will set n_estimators to quite a low value while we are using grid search, so that the algorithm doesn't take too long. Once we have found the optimum parameters, we will use a larger number of estimators for the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings; warnings.simplefilter('ignore') #prevent warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1. Split the data into training and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest algorithm allows categorical data to be used without creating dummies. Nevertheless, the scikit-learn library in Python needs all the variables to be numeric. Therefore, our categorical variables must be converted to dummy variables.\n",
    "\n",
    "\"If you hot encode a variable A with three options 'happy', 'sad', 'mad' into three binary variables x1, x2, x3, having the decision rule A == 'happy' is basically the same as x1 <= 0.5. It is computationally more expensive to do the hot encoding, but as you will notice, you will get good performance with Random Forest.\n",
    "The problem is when your categorical variable can take a lot of values; performance of the RF with this encoding tends to be worse than in R (for example) where the categorical data is handled more naturally\". Source (https://github.com/scikit-learn/scikit-learn/issues/5442)\n",
    "\n",
    "\n",
    "Some readings:\n",
    "1. https://towardsdatascience.com/random-forest-in-python-24d0893d51c0\n",
    "2. https://datascience.stackexchange.com/questions/26283/how-can-i-fit-categorical-data-types-for-random-forest-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables for categorical variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Build a Random Forest Classifier to predict if a patient is a liver patient or not. \n",
    "\n",
    "1. Use GridSearchCV to find the best 'max_depth' and 'class_weight'. \n",
    "2. Train a new classifier using that 'max_depth' and 'class_weight'. \n",
    "3. Assess the classifier in the test set: accuracy, f1 score, f1_macro, precision, recall, and AUC/ROC.\n",
    "4. Display feature importance\n",
    "5. Comment the performance of the classifier.\n",
    "6. Reply the question: wouldy you use this classifier?\n",
    "\n",
    "Tips:\n",
    "1. Follow the template of the second part of Exercise 1. In Exercise 1, we searched the best parameters in two rounds. Do only one round here, but tune the grid as many times as you need. You can give an explanation of the tuning that you followed in the space provided below.\n",
    "2. Pay attention to the categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'max_depth': [values for max_depth],\n",
    "              'class_weight': [values for class_weight]}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Parameters Tuning:</b>\n",
    "#####################################################################################################################\n",
    "\n",
    "(Double-click here)\n",
    "\n",
    "\n",
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
