{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/HDAT9500Banner.PNG)\n",
    "<br>\n",
    "\n",
    "# Chapter 4: Tree Based Methods\n",
    "\n",
    "# Assessment: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####################################################################################\n",
    "\n",
    "Double-click to write down your name and surname.\n",
    "\n",
    "**Name:**\n",
    "\n",
    "\n",
    "**Surname:**\n",
    "\n",
    "**Honour Pledge** <p>\n",
    "    \n",
    "    \n",
    "Declaration: <p>\n",
    "    \n",
    "    \n",
    "I declare that this assessment item is my own work, except where acknowledged, and has not been submitted for academic credit elsewhere or previously, or produced independently of this course (e.g. for a third party such as your place of employment) and acknowledge that the assessor of this item may, for the purpose of assessing this item: \n",
    "\n",
    "    a. Reproduce this assessment item and provide a copy to another member of the University; and/or \n",
    "    b. Communicate a copy of this assessment item to a plagiarism checking service (which may then retain a copy of the assessment item on its database for the purpose of future plagiarism checking). \n",
    "\n",
    "#####################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "In this exercise, we will introduce tree based methods. First, we will learn about the basic decision tree, then we will see how the performance of decision trees can be improved via ensemble methods - specifically, gradient descent boosting.\n",
    "\n",
    "\n",
    "## 1.1. Aims of the Exercise:\n",
    " 1. To introduce Random Forest.\n",
    " 2. To explore parameters and determine appropriate choices.\n",
    "\n",
    " \n",
    "It aligns with all of the learning outcomes of our course: \n",
    "\n",
    "1.\tDistinguish a range of task specific machine learning techniques appropriate for Health Data Science.\n",
    "2.\tDesign machine learning tasks for Health Data Science scenarios.\n",
    "3.\tConstruct appropriate training and test sets for health research data.\n",
    "\n",
    "\n",
    "## 1.2. Jupyter Notebook Intructions\n",
    "1. Read the content of each cell.\n",
    "2. Where necessary, follow the instructions that are written in each cell.\n",
    "3. Run/Execute all the cells that contain Python code sequentially (one at a time), using the \"Run\" button.\n",
    "4. For those cells in which you are asked to write some code, please write the Python code first and then execute/run the cell.\n",
    " \n",
    "## 1.3. Tips\n",
    " 1. The square brackets on the left hand side of each cell indicate whether the cell has been executed or not. Empty square brackets mean that the cell has not been executed, whereas square brackets that contain a number means that the cell has been executed. Run all the cells in sequence, using the \"Run\" button.\n",
    " 2. To edit this notebook, just double-click in each cell. In the document, each cell can be a \"Code\" cell or \"text-Markdown\" cell. To choose between these two options, go to the combo-box above. \n",
    " 3. If you want to save your notebook, please make sure you press the \"floppy disk\" icon button above. \n",
    " 4. To clean the content of all cells and re-start the Notebook, please go to Cell->All Output->Clear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load the liver patient dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patients with liver disease have been continuously increasing because of excessive consumption of alcohol, inhalation of harmful gases, etc. <p>\n",
    "\n",
    "This data set contains 416 liver patient records and 167 non liver patient records collected from North East of Andhra Pradesh, India. The \"Dataset\" column is a class label used to divide groups into liver patient (liver disease = 1) or not (no disease = 0). This data set contains 441 male patient records and 142 female patient records.<p>\n",
    "\n",
    "Any patient whose age exceeded 89 is listed as being of age \"90\".<p>\n",
    "\n",
    "Use these patient records to determine which patients have liver disease and which ones do not.<p>\n",
    "\n",
    "Attribute Information:<p>\n",
    "    1. Age: Age of the patient\n",
    "    2. Gender: Gender of the patient\n",
    "    3. Total_Bilirubin: Total Bilirubin\n",
    "    4. Direct_Bilirubin: Direct Bilirubin\n",
    "    5. Alkaline_Phosphatase: Alkaline Phosphatase\n",
    "    6. Alanine_Aminotransferase: Alanine Aminotransferase\n",
    "    7. Aspartate_Aminotransferase: Aspartate Aminotransferase\n",
    "    8. Total_Proteins: Total Proteins\n",
    "    9. Albumin: Albumin\n",
    "    10. Albumin_and_Globulin_Ratio: Albumin and Globulin Ratio\n",
    "    \n",
    "Source: https://www.kaggle.com/arslanengr/liver-patient-classification-data-analysis/data\n",
    "\n",
    "Journal Article: https://pdfs.semanticscholar.org/c92d/38a7a76c20a317de63fb9278bb10102c758b.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 1: What is the research question of this problem? (5 marks)</font>\n",
    "<p><font color='green'> Tip: Read the journal article cited above </font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Write the answer here:</b>\n",
    "#####################################################################################################################\n",
    "\n",
    "(Double-click here)\n",
    "\n",
    "\n",
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from plotnine import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liver = pd.read_csv('data/liver-data/data_Clean.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check:\n",
    "#display(liver[:][:5])\n",
    "#print(liver.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.Split the data into features and response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = liver.drop(['liver_patient'], axis = 1)\n",
    "y = liver[['liver_patient']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "# display(X[:][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "# display(y[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/ML-work-flow.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 2 - Step 2 - Preparing Data: Visualise, explore and clean the data (if necessary). (20 marks)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Python code here:\n",
    "# Minimum of 3 plots and 1 descriptive table\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Predicting Liver Patient using Random Forests\n",
    "In this section we will tune the parameters and eventually build a random forest to predict if a patient suffers from liver disease or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Parameters\n",
    "Here are some of the more important parameters:\n",
    "\n",
    "* n_estimators: Unlike Gradient Boosted Decision Trees, Random Forests *always* improve with an increased number of estimators, and there is no danger of overfitting. However, there are diminishing returns, with improvement quickly plateauing. The number of estimators are limited by our time and computational resources. <p>\n",
    "* max_features: This is the number of features to consider when looking for the best split. max_features determines how random each tree is. A large max_features means the trees will be more similar, possibly allowing for overfitting. On the other hand, a smaller max_features reduces overfitting, but may force each tree to be very deep. For classification, it is common to use the default of max_features = $\\sqrt {number\\:of\\:features}$.<p>\n",
    "    \n",
    "* class_weight: Random Forest has a parameter to penalise incorrect class labels differently. This is very useful for our imbalanced data.<p>\n",
    "    \n",
    "* max_depth: This parameter controls the maximum depth of the tree. If not specified, the nodes are expanded until all leaves are pure, or until all leaves contain less than min_samples_split samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Tuning our model\n",
    "We will use a cross-validated grid search, using GridSearchCV.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings; warnings.simplefilter('ignore') #prevent warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest algorithm allows categorical data to be used without creating dummies. Nevertheless, the scikit-learn library in Python needs all the variables to be numeric. Therefore, our categorical variables must be converted to dummy variables.\n",
    "\n",
    "\n",
    "Some readings:\n",
    "1. https://towardsdatascience.com/random-forest-in-python-24d0893d51c0\n",
    "2. https://datascience.stackexchange.com/questions/26283/how-can-i-fit-categorical-data-types-for-random-forest-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/ML-work-flow.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build a Random Forest Classifier to predict if a patient is a liver patient or not. (Step 3: Choosing a model)**\n",
    "\n",
    "<p><font color='green'>Tips:\n",
    "1. Follow the template of the second part of Exercise 1. In Exercise 1, we searched the best parameters in two rounds. Do only one round here, but tune the grid as many times as you need. You can give an explanation of the tuning that you followed in the space provided below.\n",
    "    2. Pay attention to the categorical variables.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 3 - Step 4 + Step 6 Training/Fit the model + Hyperparameter Tuning: Define and run the GridSearchCV to find the best 'max_features', 'max_depth' and 'n_estimators'. Train the model. (25 marks)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Python code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 4 - Step 5 Evaluation: Assess the classifier in the test set: accuracy, f1 score, f1_macro, precision, recall, and AUC/ROC. (20 marks)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Python code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 5 - Step 8 Interpretation: Display feature importance. (10 marks)</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Python code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 6 - Step 9 Deployment: Would you use this classifier? (20 marks)</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Parameters Tuning:</b>\n",
    "#####################################################################################################################\n",
    "\n",
    "(Double-click here)\n",
    "\n",
    "\n",
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
